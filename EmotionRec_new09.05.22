import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import confusion_matrix, classification_report
import pandas as pd
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization , Input ,concatenate
from keras.losses import categorical_crossentropy,categorical_hinge,hinge,squared_hinge
from keras.models import Model
from keras.regularizers import l2
from keras.callbacks import  EarlyStopping, ModelCheckpoint
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.python.keras import regularizers
from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping


path='C:/data3/fer2013.csv'
data=pd.read_csv(path)
data.head()

train=data.loc[data.Usage=="Training"]
Test=data.loc[data.Usage=="PrivateTest"]
val=data.loc[data.Usage=="PublicTest"]
test=pd.concat([Test,val])
X_train = np.array(list(map(str.split, train.pixels)), np.float32)
X_test = np.array(list(map(str.split, test.pixels)), np.float32)
X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)
X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)
X_train=X_train/255
X_test=X_test/255
y_train = train.emotion
y_test = test.emotion
y_train = np_utils.to_categorical(y_train, 7)
y_test = np_utils.to_categorical(y_test, 7)

print(f"X_train shape: {X_train.shape} - y_train shape: {y_train.shape}")
print(f"X_test shape: {X_test.shape} - y_test shape: {y_test.shape}")


############ Callbacks function ############
lrd = ReduceLROnPlateau(monitor='val_loss', patience=20, verbose=1, factor=0.50, min_lr=1e-10)

mcp = ModelCheckpoint('model.h5')

es = EarlyStopping(verbose=1, patience=20)

from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

METRICS = [
    tf.keras.metrics.BinaryAccuracy(name='accuracy'),
    tf.keras.metrics.Precision(name='precision'),
    tf.keras.metrics.Recall(name='recall'),
    tf.keras.metrics.AUC(name='auc'),
    tfa.metrics.F1Score(num_classes=7, name='f1_score', average='macro'),
]
classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']
def resnet_model():
    #from: https://chroniclesofai.com/transfer-learning-with-keras-resnet-50/
    model = Sequential()

    pretrained_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False,
                     input_tensor=Input(shape=(48, 48, 3)))

    for layer in model.layers:
        layer.trainable = False

    model.add(pretrained_model)
    model.add(Flatten())
    model.add(Dense(512, activation='relu'))
    model.add(Dense(5, activation='softmax'))

    model.summary()
    return model

def inception_model():
    #from: https://www.analyticsvidhya.com/blog/2021/10/understanding-transfer-learning-for-deep-learning/
    #and https://medium.com/analytics-vidhya/transfer-learning-using-inception-v3-for-image-classification-86700411251b
    pretrained_model = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=(48, 48, 3))

    # The last 15 layers fine tune
    for layer in pretrained_model.layers:
        layer.trainable = False

    x = pretrained_model.output
    x = GlobalAveragePooling2D()(x)
    x = Flatten()(x)
    x = Dense(units=512, activation='relu')(x)
    x = Dropout(0.3)(x)
    x = Dense(units=512, activation='relu')(x)
    x = Dropout(0.3)(x)
    output = Dense(units=4, activation='softmax')(x)
    model = Model(pretrained_model.input, output)

    model.summary()
    return model

def CNN_model():
    model = Sequential()
    model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48, 1)))
    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(
        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))

    model.add(Flatten())
    model.add(Dense(512, activation='relu'))
    model.add(Dropout(0.5))

    model.add(Dense(7, activation='softmax'))

    # Display the model's architecture and summarize model.
    model.summary()
    return model


# Define per-fold score containers
acc_per_fold = []
loss_per_fold = []
# Merge inputs and targets
inputs = np.concatenate((X_train, X_test), axis=0)
targets = np.concatenate((y_train, y_test), axis=0)

# Define the K-fold Cross Validator
kfold = KFold(n_splits=5, shuffle=True)

# K-fold Cross Validation model evaluation
fold_no = 0
for train, test in kfold.split(inputs, targets):
    model = inception_model() #call on CNN_model(), resnet_model() or inception_model()

    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=METRICS)
    # Generate a print
    print('------------------------------------------------------------------------')
    print(f'Training for fold {fold_no} ...')

    #steps_per_epoch = train_dataset.n // train_dataset.batch_size
    #validation_steps = valid_dataset.n // valid_dataset.batch_size

    # Fit data to model
    history = model.fit(inputs[train], targets[train],
                        batch_size=128,
                        callbacks=[lrd, mcp, es],
                        epochs=50,
                        validation_data=(inputs[test], targets[test]),                              #eller inputs[test], targets[test]/ X_test, y_test
                        verbose=1)

    plt.figure(figsize=(5, 5))
    plt.subplot(1, 2, 1)
    plt.plot(history.history["accuracy"], label="train_accuracy")
    plt.plot(history.history["val_accuracy"], label="val_accuracy")
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.title("Train and Validation accuracy Over Epochs", fontsize=14)
    plt.legend()
    plt.grid()

    plt.subplot(1, 2, 2)
    plt.plot(history.history["loss"], label="train_loss")
    plt.plot(history.history["val_loss"], label="val_loss")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Train and Validation loss Over Epochs", fontsize=14)
    plt.legend()
    plt.grid()
    plt.show()

    # Generate generalization metrics
    scores = model.evaluate(inputs[test], targets[test], verbose=0)

    print(
        f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1] * 100}%')
    loss_per_fold.append(scores[0])
    acc_per_fold.append(scores[1] * 100)
    results = model.predict(inputs[test], batch_size=128)
    predicted_classes = np.argmax(results, axis=1)
    y_classes = np.argmax(targets[test], axis=1)
    cm = confusion_matrix(y_classes, predicted_classes, normalize='true')
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)
    disp.plot()
    plt.show()

    # Increase fold number
    fold_no = fold_no + 1

model.save_weights('New4_bestweight.h5')
model.save('New4_bestmodel.h5')

# == Provide average scores == #
print('------------------------------------------------------------------------')
print('Score per fold')
for i in range(0, len(acc_per_fold)):
    print('------------------------------------------------------------------------')
    print(f'> Fold {i + 1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')
    print('------------------------------------------------------------------------')

print('Average scores for all folds:')
print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')
print(f'> Loss: {np.mean(loss_per_fold)}')
print('------------------------------------------------------------------------')

#Print Classification Report
print('Classification Report')
print(classification_report(y_classes, predicted_classes, target_names=classes))


#####################3brukes ikke###################
# repeated evaluation of a standalone model
def eval_standalone_model(X_train, y_train, X_test, y_test, n_repeats):
    acc_per_fold = []
    loss_per_fold = []
    for _ in range(n_repeats):
        # define and fit a new model on the train dataset
        model = model.fit(X_train, y_train)
        # evaluate model on test dataset
        test_acc = model.evaluate(inputs[test], targets[test], verbose=0) #eller X_test, y_test
        print(
            f'Score for evaluated standalone model: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1] * 100}%')
        loss_per_fold.append(test_acc[0])
        acc_per_fold.append(test_acc[1] * 100)
    return loss_per_fold, acc_per_fold



