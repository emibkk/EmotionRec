########### Import Modules ##############
import pathlib
import numpy as np
import matplotlib.pyplot as plt
import skimage.io
import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow.keras.preprocessing import image_dataset_from_directory
from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization, Activation, Conv2D, MaxPooling2D
from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping
from tensorflow.python.keras import regularizers
from tensorflow.keras.utils import plot_model
import os
os.environ["PATH"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'
####### Importing and analyzing dataset ##########


path_test= 'C:/data/test'
path_train= 'C:/data/train'
data_dir_test =pathlib.Path(path_test)
data_dir_train =pathlib.Path(path_train)
image_count_test = len(list(data_dir_test.glob('*/*.jpg')))
image_count_train = len(list(data_dir_train.glob('*/*.jpg')))
print("Image count test folder: ", image_count_test)
print("Image count train folder: ", image_count_train)

# load an example image
im = load_img('C:/data/train/angry/Training_33331.jpg')

# summarize some details about the image
print("Image format: ", im.format)
print("Image mode: ", im.mode)
print("Image size: ", im.size)

x = img_to_array(im)  # this is a Numpy array with shape (3, 150, 150)
x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)

# Datagen to manipulate the image data after it is loaded, including pixel scaling and data augmentation.
#Followed from source: https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/
train_datagen = ImageDataGenerator(rescale=1. / 255,
                                   validation_split=0.2,
                                   #rotation_range=5,
                                   #width_shift_range=0.2,
                                   #height_shift_range=0.2,
                                   #shear_range=0.2,
                                   horizontal_flip=True,
                                   #vertical_flip=True,
                                   #zoom_range=0.3,
                                   fill_mode='nearest')

#valid_datagen = ImageDataGenerator(rescale=1. / 255,
                                   #validation_split=0.2)

test_datagen = ImageDataGenerator(rescale=1. / 255
                                  )

train_dataset = train_datagen.flow_from_directory(directory = 'C:/data/train',
                                                  target_size = (48,48),
                                                  class_mode = 'categorical',
                                                  #subset = 'training',
                                                  shuffle=True,
                                                  color_mode='grayscale',
                                                  batch_size = 64)

#valid_dataset = valid_datagen.flow_from_directory(directory = 'C:/data/train',
#                                                  target_size = (48,48),
#                                                  class_mode = 'categorical',
#                                                  subset = 'validation',
#                                                  batch_size = 64)

test_dataset = test_datagen.flow_from_directory(directory = 'C:/data/test',
                                                target_size = (48,48),
                                                class_mode = 'categorical',
                                                shuffle=True,
                                                color_mode='grayscale',
                                                batch_size = 16)


print(train_dataset.class_indices)
# the .flow() command below generates batches of randomly transformed images
# and saves the results to the `preview/` directory
#i = 0
#for batch in train_datagen.flow(x, batch_size=1,
#                          save_to_dir='C:/preview', save_prefix='angry', save_format='jpeg'):
#    i += 1
#    if i > 20:
#        break  # otherwise the generator would loop indefinitely

# Verify generator by plotting a face and printing corresponding label
# Returning batches of image samples
img, label = train_dataset.__next__()

class_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']

# Show sample image and label
image = img[1]
labl = class_labels[label[1].argmax()]
#plt.imshow(image[:, :, 0], cmap='gray')
#plt.title(labl)
#plt.show()

######################### Define Model ###########################
#Define metrics
METRICS = [
    tf.keras.metrics.BinaryAccuracy(name='accuracy'),
    tf.keras.metrics.Precision(name='precision'),
    tf.keras.metrics.Recall(name='recall'),
    tf.keras.metrics.AUC(name='auc'),
    tfa.metrics.F1Score(num_classes=7, name='f1_score'),
]

# Initialising the CNN
model = Sequential()

model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48, 1)))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(2, 2))
model.add(Dropout(0.25))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.01)))
model.add(Conv2D(256, kernel_size=(3, 3), activation='relu', kernel_regularizer=regularizers.l2(0.01)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(7, activation='softmax'))

# Compiling model
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=METRICS)
# Display the model's architecture and summarize model.
model.summary()
#plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)



############ Callbacks function ############
lrd = ReduceLROnPlateau(monitor='val_loss', patience=20, verbose=1, factor=0.50, min_lr=1e-10)

mcp = ModelCheckpoint('model.h5')

es = EarlyStopping(verbose=1, patience=20)


# Fit the model
steps_per_epoch = train_dataset.n // train_dataset.batch_size
validation_steps = test_dataset.n // test_dataset.batch_size

history = model.fit(x=train_dataset,
                 validation_data=test_dataset,
                 epochs=60,
                 callbacks=[lrd, mcp, es],
                 steps_per_epoch=steps_per_epoch,
                 validation_steps=validation_steps)

#history = model.fit(train_dataset, validation_data=valid_dataset, epochs=2, batch_size=10, verbose=1, callbacks=[lrd, mcp, es])

#score = f1_score()
#print('F-Measure: %.3f' % score)
#model.save('saved_model.h5')

############# Plot ################
plt.figure(figsize=(14,5))
plt.subplot(1,5,1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['train', 'test'], loc='upper left')

plt.subplot(1,5,2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend(['train', 'test'], loc='upper left')

plt.subplot(1,5,3)
plt.plot(history.history['precision'])
plt.plot(history.history['val_precision'])
plt.title('Model precision')
plt.ylabel('Precision')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

plt.subplot(1,5,4)
plt.plot(history.history['recall'])
plt.plot(history.history['val_recall'])
plt.title('Model recall')
plt.ylabel('Recall')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

plt.subplot(1,5,5)
plt.plot(history.history['f1_score'])
plt.plot(history.history['val_f1_score'])
plt.title('Model F1 score')
plt.ylabel('F1 score')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

########## Confusion Matrix ######
from sklearn.metrics import classification_report, confusion_matrix
y_pred = model.predict(train_dataset)
y_pred = np.argmax(y_pred, axis=1)
class_labels = test_dataset.class_indices
class_labels = {v:k for k,v in class_labels.items()}

cm_test = confusion_matrix(train_dataset.classes, y_pred)
print('Confusion Matrix')
print(cm_test)
print('Classification Report')
target_names = list(class_labels.values())
print(classification_report(train_dataset.classes, y_pred, target_names=target_names))

plt.figure(figsize=(8,8))
plt.imshow(cm_test, interpolation='nearest')
plt.colorbar()
tick_mark = np.arange(len(target_names))
_ = plt.xticks(tick_mark, target_names, rotation=90)
_ = plt.yticks(tick_mark, target_names)

